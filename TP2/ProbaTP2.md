#TP2 Applications de la simulation probabiliste

DIDIER Thomas & GAUDIN Emma

---
#Introduction
---


L'objectif de ce TP est d'√©tudier, √† travers la simulation, les notions de convergence des variables al√©atoires (en probabilit√© et presque s√ªre) et les techniques d'int√©gration num√©rique bas√©es sur la m√©thode de Monte Carlo.

La premi√®re partie se concentre sur l'illustration, par des simulations, des concepts de convergence en probabilit√© et presque s√ªre.

Nous abordons ensuite l'Int√©gration par des m√©thodes de Monte Carlo pour estimer des int√©grales et d√©terminer les intervalles de confiance associ√©s en s'appuyant sur le TCL.



# 1- Convergence de v.a. et simulation probabiliste

## 1.1 Rappels : loi faible et forte des grands nombres

## 1.2 Illustrations des convergences en probabilit√© et p.s.

Application 1 (Estimer la valeur d‚Äôune probabilit√© par une fr√©quence)

Soit $X$ une v.a. r√©elle et $E$ un bor√©lien quelconque de $\mathbb{R}$.
Si $h(\cdot) = \mathbf{1}_{E}(\cdot)$, alors $h(X)$ suit une loi de Bernoulli de param√®tre $p := \mathbb{P}\{h(X) = 1\} = \mathbb{P}\{X \in E\}$.
Dans ce cas, la Loi Faible des Grands Nombres (LFGN) nous dit que l‚Äôon peut estimer cette probabilit√© par la fr√©quence d‚Äôobservation de valeurs dans $E$ pour un $n$-√©chantillon $X_1, \dots, X_n$ de la loi de $X$ :
$$
\overline{\mathbf{1}_{E}(X)}_n = \frac{1}{n} \sum_{k=1}^{n} \mathbf{1}_{E}(X_k(\omega))
$$
ou encore par la proportion de succ√®s sur une r√©alisation $\mathbf{1}_{E}(x_1), \dots, \mathbf{1}_{E}(x_n)$ de $\mathbf{1}_{E}(X_1), \dots, \mathbf{1}_{E}(X_n)$.

---
###Code R1: Illlustration de la convergence en probabilit√©

---

Dans cette section nous illustrerons la convergence en probabilit√©.

 Pour cela nous consid√©rons un √©chantillon $({Xn})_{n\geq 1}$ d'une loi uniforme sur [1,9]. Pour tout $\hat{b}n := 2\overline{X}_n-1$


```{r}
# code R1
gen_bn = function(n){
  X=runif(n,1,9) #G√©n√©ration de l'√©chantillon i.i.d. les Xk sont g√©n√©r√©es selon une loi uniforme sur [1, 9]
  Mn=cumsum(X)/(1:n) #calcule la moyenne empirique Mn = Sn / n (avec Sn = X1 + ... + Xn).
  b=2*Mn-1 # bn := 2Mn‚àí1.
  return(b-9)
}
#Utilisation de ConceptsConvergence
check.convergence(50000,500,gen_bn,mode="p")
```

1) Dans le code pr√©c√©dent l'appel √† la fonction check.convergence (n=50 000 et M = 500 ) permet l'obtention d'un beau graphique sur lequel on peut remarquer que pour un n tr√®s grand $\hat{p}n$ et $\hat{a}n$ tendent vers 0. On peut donc en d√©duire qu'il semble que $(\hat{b}n -9) \xrightarrow{\text{proba}} 0$ (gr√¢ce a $\hat{p}n$) et $(\hat{b}n -9) \xrightarrow{\text{ p.s.}} 0$ (gr√¢ce a $\hat{a}n$).
Ce qui revient √†
$$
\begin{align*}
(\hat{b}n) \xrightarrow{\text{proba}} 9\\
(\hat{b}n) \xrightarrow{\text{Pp.s.}} 9 \\
\end{align*}
$$

2) Montrons que $(\hat{b}n) \xrightarrow{\text{proba}} 9$ par la th√©orie:

D'apr√®s la loi faible des grands nombres on a:

Soit $({Xn})_{n\geq 1}$ une suite de v.a.r.i.i.d. admettant un moment d'ordre 1 avec $\mathbb{E}[X_1]$=m on a alors $\overline{X}_n \xrightarrow{\text{proba}} m$.

Ici les $({Xn})_{n\geq 1}$ suivent une loi uniforme sur [1,9].
On a donc
$$
\begin{aligned}
\mathbb{E}[X_1] &= \int_{-\infty}^{\infty} x f_{X_1}(x) \, dx & \quad \\
&= \int_{1}^{9} x \cdot \frac{1}{9-1} \, dx & \quad \\
&= \frac{1}{8} \left[ \frac{x^2}{2} \right]_{1}^{9} & \quad  \\
&= 5
\end{aligned}
$$

Donc $$\overline{X}_n \xrightarrow{\text{proba}} 5$$

Soit h : $\begin{align*}
\mathbb{R} \rightarrow \mathbb{R}\\  
 x \mapsto 2x -1 \\
\end{align*}
$
  ,  h est continue.

Si ${X}_n \xrightarrow{\text{proba}} X$ alors $h({X}_n) \xrightarrow{\text{proba}} h(X)$ par les propositions du cours.

Donc $$\hat{b}n:=2\overline{X}_n -1 \xrightarrow{\text{proba}} 2*5 - 1= 9$$


Ensuite montrons que nous avons aussi la convergence $\mathbb{P} p.s.$ .

Cette fois ci d'apr√®s la loi forte des grand nombre avec les m√™mes arguments: Soit $({Xn})_{n\geq 1}$ une suite de v.a.r.i.i.d. admettant un moment d'ordre 1 avec $\mathbb{E}[X_1]$=m on a alors $\overline{X}_n \xrightarrow{\text{Pp.s.}} m$. Ici les $({Xn})_{n\geq 1}$ suivent une loi uniforme sur [1,9].

On a donc $$\overline{X}_n \xrightarrow{\text{Pp.s.}} 5$$

Soit h : $\begin{align*}
\mathbb{R} \rightarrow \mathbb{R}\\  
 x \mapsto 2x -1 \\
\end{align*}
$
,h est continue.

$$\hat{b}n:=2\overline{X}_n -1 \xrightarrow{\text{Pp.s.}} 2*5 - 1= 9$$

Nos r√©sultats th√©oriques sont bien en accord avec les r√©sultats conjectur√©s gr√¢ce √† la m√©thode check.convergence


---
###Code R2: Illustration de la convergence Pp.s.

---


Dans cette section nous illustrerons la convergence Pp.s.

Pour cela nous cond√©rons un √©chantillon $({Xk})_{k\geq 1}$ d'une loi uniforme sur [0,1].

Pour tout $n\geq 1$ on pose $M_n=max_{k=1,...,n}X_k$.

On souhaite explorer la convergence p.s. de $(M_n)_{k\geq 1}$

Pour cela nous cr√©ons une fonction gen_Mn qui g√©n√®re pour n donn√©, les valeurs de $M_1, M_2, ..., M_n$


```{r}
#code R2

#gen_Mn qui g√©n√®re pour n donn√©, les valeurs de  ùëÄ1,ùëÄ2,...,ùëÄùëõ
gen_Mn=function(n){
  res=cummax(runif(n))#prend le max de n nombres al√©atoires entre [0,1]
  return (abs(res-1))# L'√©tude de |M_k - 1| vers 0 est √©quivalente √† l'√©tude de la convergence p.s.
  # de M_k vers 1, que l'on souhaite illustrer.
}

n=1000
plot (1:n,gen_Mn(n), type="l",xlab = "n", ylab = "M_n",
      main = "Plusieurs r√©alisations de M_n",
      sub = "Figure 1")#on trace la graphique de notre fonction gen_Mn en fonction des n


rep= replicate(4, gen_Mn(n))# on cr√©√© 5 r√©pliques ind√©pendantes
  # On ajoute les lignes de chaque colonne de la matrice
  color <- c("blue", "red", "yellow", "purple")
  for (i in 1:4) {
    lines(1:n, rep[, i], col = color[i]) #on ajoute nos nouvelles r√©pliques au graphique
  }

```

Sur le graphique g√©n√©r√© par le code ci dessus (Figure 1), nous voyons que les 5 repliques de Mn tendent vers 0 quand n est grand. Nous pouvons donc conjecturer que  $|M_n - 1|$  tend Pp.s. vers 0, soit que Mn tend Pp.s. vers 1. Ce r√©sultat
est assez logique puisque la suite ${Mn}_{1‚â§n‚â§100}$ repr√©sente la valeur maximale des n tirages d‚Äôune loi uniforme sur [0,1].

Nous comparons ensuite notre r√©sultat avec le r√©sultat obtenu avec la fonction check.convergence.


```{r}
#code R2
#Utilisation de ConceptsConvergence
check.convergence(1000,100,gen_Mn)
```

On voit ici sur le graphique de droite (g√©n√©rer par le package ConvergenceConcepts) que $aÃÇ_n:=M_n-1$ tend vers 0. Cela signifie que la suite $(M_n ‚àí 1)_{n‚â•1}$ converge p.s. vers
0 soit que la suite $(M_n)_{n‚â•1}$ converge $\mathbb{P}-ps$ vers 1.

En effet ce r√©sultat se prouve aussi th√©oriquement :

Soit $\{X_k\}_{k\ge 1}$ √©chantillon d‚Äôune loi uniforme sur $[0,1]$ et soit
$$Z_n := \sup_{k \ge n} |M_k - 1|, \quad n \ge 1$$
o√π $M_n := \max(X_1, \dots, X_n)$.

Montrons que $Z_n \xrightarrow{proba} 0$.

$Z_n \xrightarrow{proba} 0 \iff \forall \epsilon > 0 \quad \lim_{n \to \infty} P(|M_n - 1| \ge \epsilon)=0$.

On commence par calculer $P(|M_n - 1| \ge \epsilon)$.

Puisque $M_n \le 1$ par d√©finition ($X_k \in [0, 1]$), nous avons $M_n - 1 \le 0$. Donc, $|M_n - 1| = -(M_n - 1) = 1 - M_n$.

L'√©v√©nement $|M_n - 1| \ge \epsilon$ est √©quivalent √† $1 - M_n \ge \epsilon$, soit $M_n \le 1 - \epsilon$.

$$P(|M_n - 1| \ge \epsilon) = P(M_n \le 1 - \epsilon)$$

Puisque $M_n$ est le maximum des $X_k$, on a:

$$\begin{align*}
P(M_n \le 1 - \epsilon) &= P(\max (X_1, \dots, X_n)\le 1 - \epsilon) \\
&= P(X_1 \le 1 - \epsilon, X_2 \le 1 - \epsilon, \dots, X_n \le 1 - \epsilon)
\end{align*}$$


Comme les variables $X_k$ sont ind√©pendantes et identiquement distribu√©es (i.i.d.) de loi $U([0, 1])$ :
$$
\forall k \in \{1, \dots, n\}, \quad F_{X_k}(x) = P(X_k \le x) =
\begin{cases}
0 & \text{si } x < 0 \\
x & \text{si } 0 \le x \le 1 \\
1 & \text{si } x > 1
\end{cases}
$$

Donc :
$$P(M_n \le 1 - \epsilon) = \prod_{k=1}^n P(X_k \le 1 - \epsilon) = (1 - \epsilon)^n$$ pas ind√©pendance des variables Xk

Puisque $0 < 1 - \epsilon < 1$, on a :
$$\lim_{n \to \infty} P(|M_n - 1| \ge \epsilon) = \lim_{n \to \infty} (1 - \epsilon)^n = 0$$

Ainsi, $M_n$ converge en probabilit√© vers $1$: $M_n \xrightarrow{P} 1$.

D√©monstration de la convergence presque s√ªre (p.s.) :


Soit  $A_n(\epsilon) = \{|M_n - 1| \ge \epsilon\}$ est :
$$
\sum_{n=1}^\infty P(|M_n - 1| \ge \epsilon) = \sum_{n=1}^\infty (1 - \epsilon)^n
$$
Puisque $0 < 1 - \epsilon < 1$, cette s√©rie g√©om√©trique converge.

D'apr√®s le Lemme de Borel-Cantelli,
$$\begin{aligned}
\sum_{n=1}^\infty P(A_n) < \infty
\Rightarrow \quad & \mathbb{P}\left(\bigcap_{K=1}^{+\infty} \bigcup_{k=K}^{+\infty} A_k\right) = 0 \quad \text{(Par Borel-Cantelli)} \\
\Rightarrow \quad & \mathbb{P}(\limsup_{n \to \infty} \{|M_n - 1| \ge \epsilon\}) = 0 \\
\end{aligned}$$

Donc par d√©finition:
$$\begin{aligned}
Z_n = \sup_{k \ge n} |M_k - 1| \xrightarrow{p.s.} 0
\Rightarrow \quad & Z_n = \sup_{k \ge n} |M_k - 1| \xrightarrow{proba} 0
\end{aligned}$$

Donc par la proposition 3.3.4 on a que $$Mn\xrightarrow{p.s.} 1$$

---
###Code R3: Illustration de la  diff√©rence entre convergence en probabilit√© et Pp.s.

---



Dans cette sectio, nous illustrerons de la diff√©rence entre convergence en probabilit√© et Pp.s.

Consid√©rons la suite de v.a. ind√©pendantes $\{X_{n}\}_{n\geq 1}$ o√π $X_n$ suit la loi $Ber(1/\sqrt{n})$.

Intuitivement, on pourrait penser que $X_n\xrightarrow[]{proba} 0$ et $X_n \xrightarrow[]{\mathbb{P}-ps}0$. On cherche alors √† v√©rifier ces hypoth√®ses en utilisant le package ConvergenceConcepts, et en tra√ßant certaines r√©alisations de $X_n$ lorsque $n$ est grand.


```{r}

#code R3

#fonction qui g√©n√®re la v.a. Xn qui suit la loi  ùêµùëíùëü(1/sqrt(n)) .
Xn = function(n){
  rbinom(1, size =1, prob = 1/sqrt(n))
}

#fonction genXn qui g√©n√®re un vecteur al√©atoire (X(1+1e+5),X(2+1e+5),...,X(n+1e+5))
genXn = function(n){
  sapply((1+1e+5):(n+1e+5), Xn)
}

#Utilisation de ConceptsConvergence
check.convergence(nmax=1e+3,M=1e+3,genXn=genXn)

#plot des valeurs Xn pour 3 tirages de  la suite {Xn} lorsque n est grand
n=1e+4
plot ((1+1e+5):(n+1e+5),genXn(n), type="l",
      xlab = "n", ylab = "Xn",main = "3 tirages de la suite {Xn}")

lines((1+1e+5):(n+1e+5),genXn(n),col="red")
lines((1+1e+5):(n+1e+5),genXn(n),col="blue")
```

Nous commencons le code par l'impl√©mentation de la fonction **Xn(n)** qui g√©n√®re un simple tirage suivant la loi $X_n$. Cette fonction nous permet d'utiliser **sapply** dans la fonction **genXn(n)** qui prend un entier en entr√©e et qui renvoi un vecteur al√©atoire $(X_{1+1e+5},X_{2+1e+5},...,X_{n+1e+5})$. On √©vite alors une boucle couteuse en calcul.
Ainsi on peut observer ce qu'il se passe lorsque n est grand, sans pour autant g√©n√©rer des vecteur de trop grande dimension.

Nous appelons ensuite la fonction **check.convergence** pour visualiser les diff√©rents type de convergence. Nous tracerons aussi un graphique (Figure 2) affichant 3 tirages de la suite ${X_n}$ pour n compris entre 100000 et 101000.

On peut observer que $\hat{p}_n$ semble proche de 0, et donc qu'il y aurait bien une convergence en probabilit√©. Le graphique semble beaucoup moins convaincant quant √† la convergence presque sure, car bien que n soit grand, il y a toujours l'apparition d'√©v√©nement $[X_n=1]$.

Montrons maintenant ces hypoth√®ses de mani√®re th√©orique.

**Convergence en probabilit√© :**


$\forall \varepsilon > 0$,


$$\begin{align*}
\mathbb{P}(|X_n|\geq \varepsilon) &= \mathbb{P}(X_n=1),\, \text{car} \,X_n(\Omega)=\{0,1\}
\\&=\frac{1}{\sqrt n} \xrightarrow[n \to \infty]{}0
\end{align*}$$


Ainsi on a $X_n \xrightarrow[]{proba}0$.

**Convergence presque sure :**

On dit que $X_n \xrightarrow[]{\mathbb{P}-ps}0$ si il existe $\mathbb{Œ©}_0$ un √©v√©nement tel que $\mathbb{P}(\mathbb{Œ©}_0)=1$ et $$
\forall \omega \in \Omega_0, \quad \lim_{n \to +\infty} \, X_n(\omega) = 0
$$

Or l'existence d'un tel √©v√©nment est impossible !

En effet,
$$\sum_{k=1}^{\infty} \mathbb{P}(X_k = 1) = \sum_{k=1}^{\infty} \frac{1}{\sqrt k} = +\infty \quad \text{(d'apr√®s Riemann)}$$
D'apr√®s le lemme de Borel Cantelli on a alors $$\mathbb{P}\left(\bigcap_{n=1}^{\infty} \bigcup_{k=n}^{\infty} \{X_k = 1\}\right) = 1$$
Cela signifie que P p.s. la suite $X_n(\omega)$ prend la valeur 1 une infinit√© de fois.

Ainsi $X_n$ ne converge pas p.s. vers $0$

Cet exemple illustre bien qu'il existe une diff√©rence entre la convergence en probabilit√© et la convergence p.s. Notamment que la convergence en probabilit√© n'implique pas la convergence p.s.

## 1.3 Rappels : convergence en loi, Th√©or√®me Central Limite (TCL).

D**√©finition 1.3 (Convergence en loi)**

La suite de v.a. r√©elles $\{X_n\}_{n\geq 1}$ converge \textbf{en loi} vers la v.a. $X$, si pour tout $t$ point de continuit√© de la fonction de r√©partition $F_X$ de $X$, on a
$$
\lim_{n\to +\infty} F_{X_n}(t) = \lim_{n\to +\infty} \mathbb{P}\{X_n \leq t\} = \mathbb{P}\{X \leq t\} = F_X(t).
$$

Une cons√©quence de la LFGN : la fonction de r√©partition empirique associ√©e √† une suite $\{Y_k\}_{k\geq 1}$ de v.a.i.i.d. de m√™me loi de fonction de r√©partition $F$, est telle qu‚Äôil existe $A \subset \Omega$ tel que $\mathbb{P}(A) = 1$ et
$$
\forall t \in \mathbb{R}, \forall \omega \in A, F_M(t, \omega) := \frac{1}{M} \sum_{k=1}^{M} \mathbf{1}_{\{Y_k\leq t\}}(\omega) = \frac{1}{M} \sum_{k=1}^{M} \mathbf{1}_{]-\infty,t]}(Y_k(\omega)) \underset{M \to +\infty}{\longrightarrow} F(t).
$$
En fait, cette convergence peut √™tre renforc√©e en :

**Th√©or√®me 1.3 (Th√©or√®me de Glivenko-Cantelli)**
Pour une suite $\{Y_k\}_{k\geq 1}$ de v.a.i.i.d. de loi commune de fonction de r√©partition $F$, alors il existe $A \subset \Omega$ tel que $\mathbb{P}(A) = 1$ et
$$
\forall \omega \in A, \lim_{M\to+\infty} \sup_{t\in\mathbb{R}} \left| F_M(t, \omega) - F(t) \right| = 0. \quad (5)
$$

**Bilan:**

Dans la D√©finition 1.3, pour $n$ fix√©, on approche la fonction de r√©partition de $X_n$ (si on ne la conna√Æt pas) par la fonction de r√©partition empirique d‚Äôun $M$-√©chantillon de $X_n$, et, pour des valeurs croissantes de $n$, on compare la fonction empirique du $M$-√©chantillon de $X_n$ √† celle de $X$. L‚Äôint√©r√™t de cette m√©thode est qu‚Äôelle s‚Äôapplique quel que soit le type de v.a. consid√©r√©.

**Th√©or√®me 1.4 (Th√©or√®me Central Limite)**

Pour une suite $\{X_n\}_{n\geq 1}$ de v.a.i.i.d. admettant un moment d‚Äôordre deux avec $m := \mathbb{E}[X_1]$ et $\sigma^2 = \mathbb{V}(X_1)$, d√©finissons les v.a.
$$
S_n := \sum_{k=1}^{n} X_k, \quad Z_n := \frac{S_n - \mathbb{E}[S_n]}{\sqrt{\mathbb{V}(S_n)}} = \sqrt{n} \left( \frac{\overline{X}_n - m}{\sigma} \right).
$$
Alors la suite $\{Z_n\}_{n\geq 1}$ converge en loi vers une v.a. $Z$ de loi $N(0, 1)$, c‚Äôest √† dire
$$
\forall t \in \mathbb{R}, \lim_{n \to +\infty} F_{Z_n}(t) = \lim_{n \to +\infty} \mathbb{P}\{Z_n \leq t\} = F_Z(t) := \int_{-\infty}^{t} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right)dx.
$$

---
###Code R5: Illustration du TCL avec des lois du chi-deux

---

Dans cette section nous allons illustrer le TCL gr√¢ce √† des lois du chi-deux.

Consid√©rons une suite de v.a. indpendantes $\{X_n\}_{n \geq 1}$ avec $X_n \sim \chi^2_n$.

Montrons que $Z_n:=\frac{X_n-n}{\sqrt{2n}}$, converge en loi vers $N(0,1)$.

On prend une suite de v.a.r ind√©pendantes $\{Y_n\}_{n\geq 1}$ de loi normale centr√©e r√©duite, tel que $X_n = Y_1^2+...+Y_n^2$. Ainsi \:

$$Z_n = \frac{Y_1^2+...+Y_n^2-n}{\sqrt{2n}} = \frac{n(\frac{Y_1^2+...+Y_n^2}{n}-1)}{\sqrt2\sqrt n}=\frac{\sqrt n (\overline{Y_n^2}-1)}{\sqrt{2}}$$

On remarque que si l'on montre que $\mathbb{E}[Y_n^2]=1$ et $\mathbb{V}[Y_n^2]=2$, alors on pourra appliquer le TCL et montrer le r√©sultat souhait√©.

Pour ce faire, nous calculons la loi de la v.a.r. $U:=Y_n^2$.

Soit $I$ un interval quelconque de $\mathbb{R}$ :

\begin{align*}
\mathbb{P}(U \in I)
&= \mathbb{P}(Y_n^2 \in I)\\
&= \int_{\mathbb{R}} \mathbf{1}_{I}(y^2) f_{Y_n}(y)\,\mathrm{d}y \quad && \text{o√π } f_{Y_n}(y) \text{ est la densit√© de } Y_n \sim \mathcal{N}(0, 1) \\
&= \frac{1}{\sqrt{2\pi}} \int_{\mathbb{R}} \mathbf{1}_{I}(y^2) e^{-y^2/2}\,\mathrm{d}y \\
&= \frac{2}{\sqrt{2\pi}} \int_{0}^{+\infty} \mathbf{1}_{I}(y^2) e^{-y^2/2}\,\mathrm{d}y \quad && \text{car l'int√©grande est paire} \\
&\stackrel{u=y^2}{=} \int_{I} \frac{1}{\sqrt{2\pi u}} e^{-u/2} \mathbf{1}_{[0,+\infty[}(u)\,\mathrm{d}u
\end{align*}

On reconnait ici la fonction densit√© de la loi $Ga(\frac{1}{2},\frac{1}{2})$, on en d√©duit donc que la v.a.r $U$ suit cette loi.

Donc $\{U_n\}_{n\geq1}$ est uen suite de v.a.r.i.i.d et elle admet un moment d'ordre 2.
$U$ suit la loi Ga$Ga(\frac{1}{2},\frac{1}{2})$ on connait donc son esp√©rance: $\mathbb{E}[U]=1$ et sa variance : $\mathbb{V}[U]=2$.

On lui applique le TCL, et on obtient que $Z_n \stackrel{loi}{‚ü∂} N(0,1)$.

Illustrons √† pr√©sent ce r√©sultat √† l'aide du code suivant \:


```{r}
#code R5

#Genere la variable al√©atoire Zn
Zn = function(n){
  X = rchisq(1,df=n)
  Z = (X-n)/sqrt(2*n)
  return(Z)
}

#Genere un m-echantillon suivant la loi de Zn
vecZn_ks = function(m,n){
  return(replicate(m,Zn(n)))
}

#V√©rification de la convergence en loi par le test de Kolmogorov-Smirnov
s = 0
for (i in 1:100){
  s=s+ks.test(vecZn_ks(100,1e+6),"pnorm",mean=0,sd=1)$p.value #test de K.S.
}
p_value_moy = s/100 #calcul de la moyenne des p-value
cat("moyenne de p.value = ")
print(p_value_moy)

#Illustration graphique de la convergence en loi

#Genere un vecteur al√©atoire (Z(1+1e+5),...,Z(n+1e+5))
genvecZn = function(n){
  return(sapply((1+1e+5):(n+1e+5),Zn))
}
#utilisation du package ConvergenceConcepts
check.convergence(1e+2,1e+4,mode="L",
                  genXn=genvecZn,
                  density=FALSE,
                  probfunc=function(x){pnorm(x,0,1)},
                  )
```

Tout d'abord on impl√©mente la fonction **Zn(n)** qui prend en param√®tre un entier, et qui renvoie un unique tirage suivant la loi de $Z_n$. Puis l'on impl√©mente la fonction **vecZn_ks(m,n)** qui r√©alise un m-√©chantillon suivant la loi de $Z_n$, afin de r√©aliser un test de Kolmogorov-Smirnov. On va donc r√©aliser 100 tests de K.S., qui vont estimer la coh√©rence d'un 100-√©chantillon suivant la loi de $Z_{1e+6}$ avec une loi $N(0,1)$. La valeur moyenne de p-value √©tant bien sup√©rieure √† $0.1$, on peut conclure que les valeurs de $Z_n$ obtenues lorsque n est grand, sont coh√©rentes avec une loi normale.

Pour la deuxi√®me partie du code, on cherche √† visualiser cette convergence. Pour ce faire, on impl√©mente la fonction **genvecZn(n)** qui g√©n√®re un vecteur $(Z_{1+1e+5},Z_{2+1e+5},...,Z_{n+1e+5})$, afin d'observer la loi de $Z_n$ lorsque $n$ est grand, sans pour autant g√©n√©rer un vecteur de trop grande taille. La convergence en loi parait tout autant convaincante, puisque la fonction de r√©partition empirique, semble superposer la fonction de r√©partition th√©orique. C'est une condition √©quivalente √† la convergence en loi puisque la fonction de r√©partition de la loi normale centr√©e r√©duite est continue en tout point.
On observe √©galement cette convergence avec le plot en 3 dimensions trac√© par le package ConvergenceConcepts. En effet pour des valeurs de n qui sont grandes (de l'ordre de $10^5$) l'√©cart entre les 2 fonctions de r√©partitions (empirique et th√©orique) est faible, de l'ordre de $10^{-2}$.

## 2- Int√©gration par des m√©thodes de Monte Carlo

La LFGN permet d‚Äôobtenir des estimations d‚Äôune esp√©rance √† partir d‚Äôune simulation probabiliste donc d‚Äôobtenir une valeur approch√©e d‚Äôun certain nombre d‚Äôint√©grales ou de somme de s√©rie en fonction du caract√®re continu ou discret des v.a. concern√©es. En effet, il suffit que l‚Äôint√©grale
$$
I := \int_{\mathbb{R}} g(x) \, dx \quad (7a)
$$
avec $\int_{\mathbb{R}} |g(x)| \, dx < +\infty$ soit interpr√©t√©e comme une esp√©rance sous la loi commune de l‚Äô√©chantillon, c‚Äôest √† dire qu‚Äôelle se r√©√©crive sous la forme
$$
I = \int_{\mathbb{R}} h(x) f(x) \, dx \quad \text{o√π } h(x) := \frac{g(x)}{f(x)} \quad (7b)
$$
o√π $f$ est une densit√© de probabilit√© telle que $f(x) = 0 \Rightarrow g(x) = 0$. Noter que $\int_{\mathbb{R}} |h(x)| f(x) \, dx = \int_{\mathbb{R}} |g(x)| \, dx < +\infty$ et $I = \mathbb{E}[h(X)]$ o√π $X$ est une v.a. admettant la densit√© $f$. Il suffit donc de simuler un $n$-√©chantillon de $(X_1, \dots, X_n)$ de la loi de densit√© $f$, puis, par la LFGN (l√©gitime car $\mathbb{E}[|h(X)|] < +\infty$), d‚Äôutiliser la moyenne empirique
$$
\overline{h(X)}_n := \frac{1}{n} \sum_{k=1}^{n} h(X_k)
$$
pour estimer la valeur de l‚Äôint√©grale dans (7b). On adoptera la notation $\mathbb{E}_f$ pour signaler une esp√©rance calcul√©e relativement √† la loi de densit√© $f$.

Par ailleurs, pour mesurer la qualit√© de l‚Äôapproximation, on construit un intervalle de confiance pour $I$ sur la base du TCL (avec estimation de la variance). La construction pour une esp√©rance est donn√©e en Annexe B. Bien entendu cela requiert que
$$
\mathbb{E}_f [h(X)^2] = \int_{\mathbb{R}} \frac{g(x)^2}{f(x)^2} f(x)\,\mathrm{d}x = \int_{\mathbb{R}} \frac{g(x)^2}{f(x)}\,\mathrm{d}x < +\infty.
$$
Notons que cette condition n‚Äôest pas garantie par $\int_{\mathbb{R}} |g(x)| \, dx < +\infty$ ou $\int_{\mathbb{R}} |g(x)|^2 \, dx < +\infty$. Sous la condition $\mathbb{E}_f [h(X)^2] < +\infty$, la variance de $\overline{h(X)}_n$ est
$$
\mathbb{V}(\overline{h(X)}_n) = \frac{\mathbb{V}_f (h(X_1))}{n} = \frac{1}{n}\left[ \int_{\mathbb{R}} h(x)^2 f(x) \, dx - I^2 \right].
$$
Le TCL permet d‚Äô√©crire
$$
\forall t \in \mathbb{R}, \lim_{n \to +\infty} \mathbb{P}\left( \frac{\sqrt{n} (\overline{h(X)}_n - I)}{\sqrt{\mathbb{V}_f(h(X_1))}} \leq t \right) = \int_{-\infty}^{t} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right) dx
$$
d‚Äôo√π l‚Äôintervalle de confiance de niveau $0.95$ avec un √©chantillon de taille $n$
$$
\text{IC}_{n,0.95}(I) = \left[ \overline{h(X)}_n \pm 1.96\sqrt{\frac{\mathbb{V}_f (h(X_1))}{n}} \right].
$$
Comme la valeur de $I$ est inconnue, la valeur de $\mathbb{V}(h(X_1))$ l‚Äôest √©galement. On la remplace alors par son estimation classique par la variance empirique
$$
S^2_n(h) = \overline{h(X)^2}_n - (\overline{h(X)}_n)^2
$$
ce qui donne, par le TCL avec estimation consistante de la variance, l‚Äôintervalle de confiance de $I$
$$
\text{IC}_{n,0.95}(I) = \left[ \overline{h(X)}_n \pm 1.96\sqrt{\frac{S^2_n(h)}{n}} \right].
$$

---
###Code R6: Exemple jouet

---

Consid√©rons l‚Äôexemple jouet de la fonction $g(x) := [ cos(50x) + sin(20x) ]^2$ et l‚Äôint√©grale

$$
\begin{aligned}
I &= \int_{0}^{1} g(x) \, dx = \int_{0}^{1} [\cos(50x) + \sin(20x)]^2 \, dx
\end{aligned}
$$

Cette derni√®re se r√©√©crit sous la forme:

$$
\begin{aligned}
    I := \int_{\mathbb{R}} g(x)\mathbf{1}_{[0,1]}(x) \, dx = \int_{\mathbb{R}} g(x)f(x) \, dx
\end{aligned}
$$
o√π f est la densit√© d‚Äôune loi unif. sur l‚Äôintervalle [0, 1]. Il suffit donc de produire un n-√©chantillon
X1, . . . , Xn d‚Äôune telle loi et d‚Äôestimer l‚Äôint√©grale par la somme
$$
\begin{aligned}
    \overline{h(X)}_{n} = \frac{1}{n} \sum_{k=1}^{n} h(X_{k}).
\end{aligned}
$$



Nous allons ensuite construire des intervalles de confiances associ√©s √† chaque estimation obtenue. On construit un intervalle de confiance pour I
sur la base du TCL (avec estimation de la variance).
$$
\begin{aligned}
\mathbb{E}_f [h(X)^2] &= \int_{\mathbb{R}} \frac{g(x)^2}{f(x)^2} f(x) \, dx \\
&= \int_{\mathbb{R}} \frac{g(x)^2}{f(x)} \, dx \\
&= \int_{0}^{1} \frac{g(x)^2}{1} \, dx \quad & \text{(Car } f(x) = \mathbf{1}_{[0,1]}(x) \text{)} \\
&= \int_{0}^{1} g(x)^2 \, dx
\end{aligned}
$$

Or la fonction $g(x)^2$ est continue sur l'intervalle ferm√© et born√© $[0, 1]$ (car somme de deux fonction continues).

Donc l'int√©grale $\int_{0}^{1} g(x)^2 \, dx$ est finie.

Donc $\mathbb{E}_f [h(X)^2]<{\infty}$. Donc $h(X)$ admet un moment d'ordre 1, et donc $I<{\infty}$.

Sous la condition $\mathbb{E}_f [h(X)^2] < +\infty$, la variance de $\overline{h(X)}_n$ est
$$
\mathbb{V}(\overline{h(X)}_n) = \frac{\mathbb{V}_f(h(X_1))}{n} = \frac{1}{n}\left[ \int_{\mathbb{R}} h(x)^2 f(x) \, dx - I^2 \right].
$$
Le TCL permet d'√©crire
$$
\forall t \in \mathbb{R}, \lim_{n \to +\infty} \mathbb{P}\left\{ \frac{\sqrt{n}(\overline{h(X)}_n - I)}{\sqrt{\mathbb{V}(h(X_1))}} \leq t \right\} = \int_{-\infty}^{t} \frac{1}{\sqrt{2\pi}} \exp(-x^2/2) \, dx
$$
d'o√π l'intervalle de confiance de niveau $0.95$ avec un √©chantillon de taille $n$
$$
\text{IC}_{n,0.95}(I) = \left[ \overline{h(X)}_n \pm 1.96\sqrt{\frac{\mathbb{V}_f(h(X_1))}{n}} \right].
$$

Comme la valeur de $I$ est inconnue, la valeur de $\mathbb{V}(h(X_1))$ l'est √©galement. On la remplace alors par son estimation classique par la variance empirique
$$
S^2_{n}(h) = \overline{h(X)^2}_n - (\overline{h(X)}_n)^2
$$
ce qui donne, par le TCL avec estimation consistante de la variance, l'intervalle de confiance de $I$
$$
\text{IC}_{n,0.95}(I) = \left[ \overline{h(X)}_n \pm 1.96\sqrt{\frac{S^2_n(h)}{n}} \right].
$$


Apr√®s avoir d√©montr√©, √† l'aide de l'exemple jouet (Code R6), l'efficacit√© de la m√©thode de Monte Carlo et la validit√© des intervalles de confiance obtenus gr√¢ce au TCL pour une fonction √† support born√© (l'intervalle $[0, 1]$) o√π la condition $\mathbb{E}_f [h(X)^2] < +\infty$ est facilement v√©rifi√©e, nous passons maintenant √† l'√©tude d'une int√©grale $I$ sur un support non born√© ($\mathbb{R}$). L'objectif du Code R7 est de tester l'estimation de cette int√©grale $I := \int_{\mathbb{R}} \frac{1}{1+|x|^3} \, dx$ en utilisant deux densit√©s diff√©rentes (Loi de Cauchy et Loi de Laplace), afin d'illustrer l'importance critique de la condition de variance finie $\mathbb{E}_f [h(X)^2] < +\infty$ pour l'application rigoureuse du TCL et la validit√© des intervalles de confiance.

---
###Code R7: Fonction √† support non born√©
---

On souhaite dans cette section calculer l'int√©grale
$$
I := \int_{\mathbb{R}} \frac{1}{1 + |x|^3} \, dx.
$$
Puis nous proposerons une estimation de $I$ en introduisant:

1.une densit√© de la loi de Cauchy : $f(x) := \frac{1}{\pi(1 + x^2)}$

2.une densit√© de type Laplace (voir Code R5 du TP N¬∞ 1) : $f(x) := \frac{1}{2} e^{- |x|}$.

On cherche aussi √† √©tudier la possibilit√© de construire un intervalle de confiance pour ces deux cas de figure.

**Commencons avec la loi de Cauchy.**

Tout d'abord, v√©rifions que $f(x) = 0 \Rightarrow g(x) = 0$ pour que $h(x) := g(x)/f(x)$ soit bien d√©fini lors de l'int√©gration par la m√©thode de Monte Carlo.

Cette condition est rapidement satisfaite car la densit√© de Cauchy $f(x) = \frac{1}{\pi(1 + x^2)}$ est strictement positive sur $\mathbb{R}$.

Afin de garantir un intervalle de confiance non erron√©, nous devons v√©rifier la condition essentielle pour le TCL, √† savoir que l'esp√©rance du carr√© de $h(X)$ est finie : $\mathbb{E}_f [h(X)^2]<{\infty}$4.

Nous obtenons :
$$
\mathbb{E}_f [h(X)^2] = \int_{\mathbb{R}} \frac{\left(\frac{1}{1 + |x|^3}\right)^2}{\frac{1}{\pi(1 + x^2)}} \, dx = \int_{\mathbb{R}} \frac{\pi(1 + x^2)}{(1 + |x|^3)^2} \, dx
$$
Pour √©tudier la convergence, nous examinons le comportement de l'int√©grande √† l'infini :
$$
\frac{\pi(1 + x^2)}{(1 + |x|^3)^2} \underset{|x| \to \infty}{\sim} \frac{\pi x^2}{(|x|^3)^2} = \frac{\pi x^2}{x^6} = \frac{\pi}{x^4}
$$

Or $\int_{\mathbb{R}} \frac{1}{x^4} \, dx$ est convergente d'apr√®s Riemann.
Donc l'int√©grale $\mathbb{E}_f [h(X)^2]$ est √©galement convergente. Nous pouvons donc valider l'utilisation du Th√©or√®me Central Limite pour construire l'intervalle de confiance.


```{r}
#code R7
g=function(x){
  return (1/(1+abs(x)^3))
}

h=function(x){
  return(g(x)/dcauchy(x))
}

IC=function(n){
  X=rcauchy(n)# G√©n√©ration des √©chantillons (X) selon la loi q(x) = Cauchy
  I=cumsum(h(X))/(1:n) #estimation de l'esp√©rance E(h(X))
  Var=pmax(0,(cumsum(h(X)^2)/(1:n) -I^2)) #estimation de la variance Var(h(X))
  Err=1.96*sqrt(Var/(1:n)) # Calcul de l'Erreur
  return(list(Moy=I, Err=Err))
}
n=1e4
indices=1:n
indices_CI=seq(1, n, length.out = 100)# D√©finition des indices pour le trac√© des IC

R√©sultat_IC=IC(n)

Moyenne=R√©sultat_IC$Moy[indices]
Error=R√©sultat_IC$Err[indices]

val_exacte = integrate(f = g, lower = -Inf, upper = Inf);# Calcul de la valeur exacte

cat("Valeur approch√©e = ")
print(Moyenne[length(Moyenne)]) # approximation finale
cat("\nValeur exacte = ")
print(val_exacte) #valeur exacte

#Trac√© Convergence de la Moyenne Estim√©e
plot(indices, Moyenne, type='l', col="blue",
     xlab = "Nombres de tirages n", ylab="Estimation de l'int√©grale I",
     ylim = c(2.2,2.6),
     main="Convergence de I pour la loi de Cauchy",
     sub = "Figure 3")

#Trac√© de la valeur exacte
abline (h=val_exacte$value, col="red")

#Trac√© des Intervalles de Confiance (IC)
plotCI(indices_CI, R√©sultat_IC$Moy[indices_CI],
       uiw=R√©sultat_IC$Err[indices_CI],liw=R√©sultat_IC$Err[indices_CI],
       add=TRUE,lwd=1,pch=NA)

#legende
legend("topright", legend = c("Moyenne Estim√©e", "Valeur Th√©orique", "Intervalle de Confiance"),
       col = c("blue", "red", "black"),
       lty = c(1, 1, 1),lwd = c(1, 1, 1), cex = 0.8)
```

Sur le graphique de la Figure 3, on retrouve bien que plus n est grand, plus l'intervalle de confinace diminue et l'estimation devient donc de plus en plus pr√©cise. Cette estimation converge vers une valeur de I repr√©sent√©e en rouge. On peut donc estimer la valeur de I √† 2.418399 (calcul√©e gr√¢ce √† la fonction integrate). Observons maintenant lorsque n est plus grand, pour cela on se muni du code suivant.


```{r}
#code R7
#m√™me chose que le programmee pr√©c√©dent mais avec n plus grand, on fait un "zoom"

g=function(x){
  return (1/(1+abs(x)^3))
}

h=function(x){
  return(g(x)/dcauchy(x))
}

IC=function(n){
  X=rcauchy(n)# G√©n√©ration des √©chantillons (X) selon la loi q(x) = Cauchy
  I=cumsum(h(X))/(1:n) #estimation de l'esp√©rance E(h(X))
  Var=pmax(0,(cumsum(h(X)^2)/(1:n) -I^2)) #estimation de la variance Var(h(X))
  Err=1.96*sqrt(Var/(1:n)) # Calcul de l'Erreur
  return(list(Moy=I, Err=Err))
}
n=1e6
indices=1:n
indices_CI=seq(1, n, length.out = 100)# D√©finition des indices pour le trac√© des IC

R√©sultat_IC=IC(n)

Moyenne=R√©sultat_IC$Moy[indices]
Error=R√©sultat_IC$Err[indices]

val_exacte = integrate(f = g, lower = -Inf, upper = Inf);# Calcul de la valeur exacte

cat("Valeur approch√©e = ")
print(Moyenne[length(Moyenne)]) # approximation finale
cat("\nValeur exacte = ")
print(val_exacte) #valeur exacte

#Trac√© Convergence de la Moyenne Estim√©e
plot(indices, Moyenne, type='l', col="blue",
     xlab = "Nombres de tirages n", ylab="Estimation de l'int√©grale I",
     ylim=c(max(c(min((Moyenne[(0.5*n):n]-Error[(n*0.5):n])),2.2)),
            min(c(max(Moyenne[(n*0.5):n]+Error[(n*0.5):n]),2.8))),
     xlim=c(n*0.5,n),
     main="Convergence de I pour la loi de Cauchy",
     sub = "Figure 4")

#Trac√© de la valeur exacte
abline (h=val_exacte$value, col="red")

#Trac√© des Intervalles de Confiance (IC)
plotCI(indices_CI, R√©sultat_IC$Moy[indices_CI],
       uiw=R√©sultat_IC$Err[indices_CI],liw=R√©sultat_IC$Err[indices_CI],
       add=TRUE,lwd=1,pch=NA)

#legende
legend("topright", legend = c("Moyenne Estim√©e", "Valeur Th√©orique", "Intervalle de Confiance"),
       col = c("blue", "red", "black"),
       lty = c(1, 1, 1),lwd = c(1, 1, 1), cex = 0.8)

```

Sur cette Figure 4, on observe que la valeur approch√© devient proche de la valeur exacte, qui est d'ailleurs contenue dans les intervales de confiance. La m√©thode donne une pr√©cision relativement convenable, puisque pour n=$10^6$, l'erreur commise reste tol√©rable, pour un temps de calcul tout √† fait raisonnable.

Dans les deux codes pr√©c√©dents, nous impl√©mentons d'abord les fonctions n√©cessaires √† la m√©thode de Monte Carlo par √©chantillonnage pr√©f√©rentiel. La fonction g(x) d√©finit l'int√©grande, $g(x) = \frac{1}{1+|x|^3}$, tandis que h(x) d√©finit la fonction transform√©e, $h(x) = g(x)/f(x)$, o√π $f(x)$ est la densit√© de la Loi de Cauchy (dcauchy(x)), pour reformuler l'int√©grale $I$ en une esp√©rance $\mathbb{E}_f[h(X)]$.

Puis, nous impl√©mentons la fonction IC(n), qui r√©alise le c≈ìur du processus de simulation. Pour un $n$ donn√©, cette fonction g√©n√®re un $n$-√©chantillon $X$ selon la loi de Cauchy (rcauchy(n)), et renvoie la suite des estimations cumul√©es de la moyenne $\overline{h(X)}_k$ et les marges d'erreur correspondantes pour les Intervalles de Confiance (IC) √† $95\%$, calcul√©es √† partir de la variance empirique.

Pour la deuxi√®me partie du code, nous cherchons √† visualiser et valider cette convergence en utilisant un grand nombre de tirages ($n=10^4$). La convergence de l'estimation de $I$ (la courbe bleue) vers la valeur exacte (la ligne rouge, $I \approx 2.418$) est trac√©e. La convergence s'av√®re convaincante, puisque la courbe de la moyenne empirique se rapproche fortement √† la valeur th√©orique lorsque $n$ grandit.

De plus, les Intervalles de Confiance (IC) trac√©s se comportent comme attendu : ils se r√©tr√©cissent (Figure 3), et la valeur exacte $I$ est constamment contenue dans l'intervalle lorsque $n$ est suffisamment grand comme on peut le constater sur la Figure 4.

Ce comportement valide l'application du Th√©or√®me Central Limite (TCL) dans ce contexte d'int√©gration par Monte Carlo avec la densit√© de Cauchy. Le succ√®s de cette simulation est en parfait accord avec l'analyse th√©orique initiale qui a confirm√© que la condition de variance finie $\mathbb{E}_f [h(X)^2] < \infty$ √©tait bien satisfaite.

**Ensuite nous √©tudions la loi de Laplace.**

Les m√™mes v√©rifications s'imposent. La premi√®re, concernant l'implication $f(x) = 0 ‚áí g(x) = 0$ est satisfaite par le m√™me argument. Nous devont alors v√©rifier que $\mathbb{E}_f [h(X)^2] < \infty$.
$$
\mathbb{E}_f [h(X)^2] = \int_{\mathbb{R}} \frac{\left(\frac{1}{1 + |x|^3}\right)^2}{\frac{1}{2} e^{-|x|}} \, dx = \int_{\mathbb{R}} \frac{2 e^{|x|}}{(1 + |x|^3)^2} \, dx
$$
Pour √©tudier la convergence, nous examinons le comportement de l'int√©grande √† l'infini :
$$
\frac{2 e^{|x|}}{(1 + |x|^3)^2}  \underset{|x| \to \infty}{\sim} +‚àû
$$

Il n'est donc pas possible de construire un intervalle de confiance pour une densit√© de type Laplace et les intervalles de confiance construits formellement seraient invalides ou trompeurs.


Nous allons donc confirmer cette affirmation par le biais visuel utilisant notre programme pr√©c√©dent mais cette fois ci avec une densit√© de type laplace.


```{r}
#code R7

g=function(x){
  return (1/(1+abs(x)^3))#fonction √† int√©grer
}

h=function(x){
  return(2*g(x)/exp(-abs(x))) #avec densit√© de la loi de Laplace pour a=1
}

#On utilise la m√©thode de l'inverse pour g√©n√©rer un n-√©chantillon de loi de Laplace
rlaplace=function(n){
  if(floor(n)!=n){
    stop("n doit etre un entier")
  }
  u=runif(n)
  return((u<(1/2))*log(2*u)-(u>=(1/2))*log(2*(1-u)))
}

IC=function(n){
  X=rlaplace(n)# G√©n√©ration des √©chantillons (X) selon la loi Laplace
  I=cumsum(h(X))/(1:n) #estimation de l'esp√©rance E(h(X))
  Var=pmax(0,(cumsum(h(X)^2)/(1:n) -I^2)) #estimation de la variance Var(h(X))
  Err=1.96*sqrt(Var/(1:n)) # Calcul de l'Erreur
  return(list(Moy=I, Err=Err))
}
n=1e4
indices=1:n
indices_CI=seq(1, n, length.out = 100)# D√©finition des indices pour le trac√© des IC

R√©sultat_IC=IC(n)

Moyenne=R√©sultat_IC$Moy[indices]
Error=R√©sultat_IC$Err[indices]

val_exacte = integrate(f = g, lower = -Inf, upper = Inf);# Calcul de la valeur exacte

cat("Valeur approch√©e = ")
print(Moyenne[length(Moyenne)]) # approximation finale
cat("\nValeur exacte = ")
print(val_exacte) #valeur exacte

#Trac√© Convergence de la Moyenne Estim√©e
plot(indices, Moyenne, type='l', col="blue",
     xlab = "Nombres de tirages n", ylab="Estimation de l'int√©grale I",
     ylim = c(2.3,2.5),
     main="Convergence de I pour la loi de Laplace",
     sub = "Figure 5")

#Trac√© de la valeur exacte
abline (h=val_exacte$value, col="red")

#legende
legend("topright", legend = c("Moyenne Estim√©e", "Valeur Th√©orique"),
       col = c("blue", "red"),
       lty = c(1, 1, 1),lwd = c(1, 1, 1), cex = 0.8)

```

Malgr√© l'√©chec th√©orique du TCL, la courbe bleue de la Moyenne (Figure 5) tend toujours vers la valeur exacte $I \approx 2.418$ lorsque $n$ est grand. Cette convergence est garantie par la Loi Forte des Grands Nombres (LFGN), qui ne requiert que $\mathbb{E}[|h(X)|] < +\infty$ (ce qui est vrai ici).


```{r}
#code R7
g=function(x){
  return (1/(1+abs(x)^3))#fonction √† int√©grer
}

h=function(x){
  return(2*g(x)/exp(-abs(x))) #avec densit√© de la loi de Laplace pour a=1
}

#On utilise la m√©thode de l'inverse pour g√©n√©rer un n-√©chantillon de loi de Laplace
rlaplace=function(n){
  if(floor(n)!=n){
    stop("n doit etre un entier")
  }
  u=runif(n)
  return((u<(1/2))*log(2*u)-(u>=(1/2))*log(2*(1-u)))
}

IC=function(n){
  X=rlaplace(n)# G√©n√©ration des √©chantillons (X) selon la loi q(x) = Cauchy
  I=cumsum(h(X))/(1:n) #estimation de l'esp√©rance E(h(X))
  Var=pmax(0,(cumsum(h(X)^2)/(1:n) -I^2)) #estimation de la variance Var(h(X))
  Err=1.96*sqrt(Var/(1:n)) # Calcul de l'Erreur
  return(list(Moy=I, Err=Err))
}
n=1e6
indices=1:n
indices_CI=seq(1, n, length.out = 100)# D√©finition des indices pour le trac√© des IC

R√©sultat_IC=IC(n)

Moyenne=R√©sultat_IC$Moy[indices]
Error=R√©sultat_IC$Err[indices]

val_exacte = integrate(f = g, lower = -Inf, upper = Inf);# Calcul de la valeur exacte

cat("Valeur approch√©e = ")
print(Moyenne[length(Moyenne)]) # approximation finale
cat("\nValeur exacte = ")
print(val_exacte) #valeur exacte

#Trac√© Convergence de la Moyenne Estim√©e
plot(indices, Moyenne, type='l', col="blue",
     xlab = "Nombres de tirages n", ylab="Estimation de l'int√©grale I",
     ylim=c(max(c(min((Moyenne[(0.5*n):n]-Error[(n*0.5):n])),2.2)),
            min(c(max(Moyenne[(n*0.5):n]+Error[(n*0.5):n]),2.8))),
     xlim=c(n*0.5,n),
     main="Convergence de I pour la loi de Cauchy",
     sub = "Figure 6")

#Trac√© de la valeur exacte
abline (h=val_exacte$value, col="red")

#Trac√© des Intervalles de Confiance (IC)
plotCI(indices_CI, R√©sultat_IC$Moy[indices_CI],
       uiw=R√©sultat_IC$Err[indices_CI],liw=R√©sultat_IC$Err[indices_CI],
       add=TRUE,lwd=1,pch=NA)

#legende
legend("topright", legend = c("Moyenne Estim√©e", "Valeur Th√©orique", "Intervalle de Confiance"),
       col = c("blue", "red", "black"),
       lty = c(1, 1, 1),lwd = c(1, 1, 1), cex = 0.8)

```

Gr√¢ce √† la figure 6 on constate que la valeur exacte n'est pas toujours contenue dans l'intervalle de confiance calcul√©. Ce comportement irr√©gulier et la non-contenance de la valeur vraie, observ√©s avec un $n$ tr√®s grand, confirment que la m√©thode des IC est erron√©e dans ce cas, car la condition $\mathbb{E}_f [h(X)^2] < +\infty$ n'est pas respect√©e.

En r√©sum√©, la LFGN garantit que $\overline{h(X)}_n \xrightarrow{p.s.} I$, mais l'infinit√© de la variance emp√™che l'application du TCL, rendant la construction des intervalles de confiance invalide.


---
###Code R8: Echantillonage pr√©f√©rentiel et estimation de petite probabilit√©
---

Dans cette ultime partie, nous cherchons √† estimer $p=\mathbb{P}(X>4)$ avec $X \sim N(0,1) $.

Le calcul de $p$ n'est en r√©alit√© qu'un calcul d'int√©grale puisque $X$ admet une densit√© \:

$$p = \int_{4}^{+\infty} \frac{1}{\sqrt{2\pi}} e^{-x^{2}/2} \, dx = \int_{\mathbb{R}} 1_{\{x > 4\}} \, \frac{1}{\sqrt{2\pi}} e^{-x^{2}/2} \, dx $$

On pose conform√©ment √† l'√©quation (7a) $g(x):=1_{]4,+\infty[}(x) \, \frac{1}{\sqrt{2\pi}} e^{-x^{2}/2}$.

g est bien int√©grable sur $\mathbb{R}$ car $\int_{\mathbb{R}} g(x) \, dx=p\leq1$ ($p$ est une probabilit√©).

Pour la suite, on choisit naturellement $f(x)$ √©gale √† la densit√© de la loi $N(0,1)$.

Notons que ce choix satisfait bien l'implication $f(x)=0 \Rightarrow
g(x)=0$. Ce qui nous permet de poser $h(x):=\frac{g(x)}{f(x)}=1_{]4,+\infty[}(x)$.

 Ainsi le calcul de $p$ revient au calcul de l'esp√©rance $\mathbb{E}[h(X)]$, o√π $X \sim N(0,1)$.

Int√©ressons nous √† la construction d'un interval de confiance qui nous permettrait de mesurer la qualit√© de notre estimation. Pour pouvoir construire un tel objet, il est n√©cessaire que $\mathbb{E}[{h(X)}^2]< + \infty $. Cette condition est imm√©diatement satisfaite puisque ${h(x)}^2=h(x)$.

Il nous est √† pr√©sent possible d'impl√©menter une premi√®re estimation de p par la m√©thode de Monte Carlo.


```{r}
#code R8
h=function(x){
  return(as.numeric(x>4))
}

#Fonction qui renvoie l'estimation de p et les intervales de confiances 0.95
IC=function(n){
  X=rnorm(n) # G√©n√©ration de n tirages i.i.d. N(0,1)
  I=cumsum(h(X))/(1:n) # Moyenne empirique E(h(X))
  Var=(cumsum(h(X)^2)/(1:n) -I^2) # Variance emprique Var(h(X))
  Err=1.96*sqrt(Var/(1:n)) #Calcul de l'Erreur
  return(list(Moy=I, Err=Err))
}

n=1e6
indices=1:n
indices_CI=seq(1, n, length.out = 100)

R√©sultat_IC=IC(n)

Moyenne=R√©sultat_IC$Moy
Error=R√©sultat_IC$Err

val_exacte = pnorm(q=4,lower.tail = F)# Calcul de la valeur exacte

cat("Valeur approch√©e = ")
print(Moyenne[length(Moyenne)]) # approximation finale
cat("\nValeur exacte = ")
print(val_exacte) #valeur exacte

plot(indices, Moyenne, type='l', col="blue",
     ylim=c(max(c(min((Moyenne[(0.5*n):n]-Error[(n*0.5):n])),-0.5)), min(c(max(Moyenne[(n*0.5):n]+Error[(n*0.5):n]),0.5))),
     xlim=c(n*0.5,n),
     xlab = "Nombres de tirages n", ylab="Approximation de p",
     main="Approximation de p \nen fonction de n",
     sub = "Figure 7")

#Trac√© de la valeur exacte
abline (h=val_exacte, col="red")

#Trac√© des Intervalles de Confiance 0.95
plotCI(indices_CI, Moyenne[indices_CI],
       uiw=Error[indices_CI],liw=Error[indices_CI],
       add=TRUE,lwd=1,pch=NA)

#legende
legend("topright", legend = c("Valeur Estim√©e", "Valeur Th√©orique", "Intervalle de Confiance"),
       col = c("blue", "red", "black"),
       lty = c(1, 1, 1),lwd = c(1, 1, 1), cex = 0.8)
```

Ce code commence par imp√©menter la fonction h(x) utilis√© pour cette m√©thode, puis par la fonction IC(n) permettant d'estimer $p$ et de calculer les intervalles de confiance associ√©s √† ces approimations. Le code affiche ensuite la valeur approch√©e puis la valeur exacte, ainsi que le graphique Figure 7. Ce graphique contient l'estimation, la valeur exacte, et les intervalles de confiance.

Le constat que l'on peut faire de la Figure 7 est clair, la convergence de cette approximation est lente. On se rend compte que m√™me pour un n tr√®s grand, il n'est pas possible d'obtenir une tr√®s bonne approximation de p en un temps raisonnable. Il est donc n√©cessaire de changer de m√©thode. Il nous ai propos√© d'√©chantilloner suivant la loi d'une v.a. $Y:=E+4$ avec $E\sim Exp(1)$.

L'id√©e va donc √™tre de prendre la fonction densit√© de $Y$ comme fonction $f$ dans (7b), on pourra alors exprimer $h(x)$ puisque $g(x)=1_{]4,+\infty[}(x) \, \frac{1}{\sqrt{2\pi}} e^{-x^{2}/2}$ (voir d√©but partie codeR8).

Soit $t \in \mathbb{R}$,
\begin{align*}
\mathbb{P}(Y\leq t) &= \mathbb{P}(E\leq t -4) = \int_{-\infty}^{t-4}e^{-x}1_{[0,+\infty[}(x)\,dx
\\&\stackrel{x=y-4}{=} \int_{-\infty}^{t}e^{-y+4}1_{[4,+\infty[}(y)\,dy
\end{align*}

D'o√π le fait que l'on prenne $f(x)=e^{-x+4}1_{[4,+\infty[}(x)$. Cette fonction $f$ v√©rifiant bien l'implication \: $f(x)=0 \Rightarrow g(x)=0$, on peut √©crire :
\
$$h(x)=\frac{g(x)}{f(x)}=\frac{1}{\sqrt{2\pi}} e^{-\frac{x^{2}}{2}+x-4}$$

On va donc appliquer la m√©thode de Monte Carlo de mani√®re analogue √† pr√©c√©demment, cependant v√©rifions en amont que $\mathbb{E}[h(Y)^2]<+\infty$:

$$h(y)^2 = \frac{1}{2\pi} (e^{-\frac{y^{2}}{2}+y-4})^2 = \frac{1}{2\pi} (e^{-\frac{y^{2}}{2}+y-4})^2=\frac{1}{2\pi} e^{-y^2+2y-8}$$

\begin{align*}
\mathbb{E}[h(y)^2]
&= \int_{\mathbb{R}} h(y)^2 f(y)\,\mathrm{d}y \\
&= \int_{4}^{+\infty} \frac{1}{2\pi} e^{-y^2 + 2y - 8} e^{-y + 4}\,\mathrm{d}y \\
&= \int_{4}^{+\infty} \frac{1}{2\pi} e^{-y^2 + y - 4}\,\mathrm{d}y < +\infty
\end{align*}

On peut ainsi construire des intervalles de confiance.


```{r}
#code R8

h=function(x){
  y=-(x^2)/2 +x -4
  return(1/sqrt(2*pi)*exp(y))
}

#Fonction qui renvoie l'estimation de p et les intervales de confiances 0.95
IC=function(n){
  X=4+rexp(n) # G√©n√©ration de n tirages i.i.d. E(1)+4
  I=cumsum(h(X))/(1:n) # Moyenne empirique E(h(X))
  Var=(cumsum(h(X)^2)/(1:n) -I^2) # Variance emprique Var(h(X))
  Err=1.96*sqrt(Var/(1:n)) #Calcul de l'Erreur
  return(list(Moy=I, Err=Err))
}

n=1e5
indices=1:n
indices_CI=seq(1,n, length.out = 100) # indices des intervales pour le graphique

R√©sultat_IC=IC(n)

Moyenne=R√©sultat_IC$Moy
Error=R√©sultat_IC$Err

val_exacte = pnorm(q=4,lower.tail = F)# Calcul de la valeur exacte

cat("Valeur approch√©e = ")
print(Moyenne[length(Moyenne)]) # approximation finale
cat("\nValeur exacte = ")
print(val_exacte) #valeur exacte

plot(indices, Moyenne, type='l', col="blue",
     ylim=c(max(c(min((Moyenne[(0.5*n):n]-Error[(n*0.5):n])),-0.5)), min(c(max(Moyenne[(n*0.5):n]+Error[(n*0.5):n]),0.5))),
     xlim=c(n*0.5,n),
     xlab = "Nombres de tirages n", ylab="Approximation de p",
     main="Approximation de p \nen fonction de n",
     sub = "Figure 8")

#Trac√© de la valeur exacte
abline (h=val_exacte, col="red")

#Trac√© des Intervalles de Confiance 0.95
plotCI(indices_CI, Moyenne[indices_CI],
       uiw=Error[indices_CI],liw=Error[indices_CI],
       add=TRUE,lwd=1,pch=NA)

#legende
legend("topright", legend = c("Valeur Estim√©e", "Valeur Th√©orique", "Intervalle de Confiance"),
       col = c("blue", "red", "black"),
       lty = c(1, 1, 1),lwd = c(1, 1, 1), cex = 0.8)
```

Ce code est impl√©ment√© de fa√ßon tout √† fait analogue au pr√©c√©dent, int√©grant les changements d√ª √† la diff√©rence d'approximation. Ainsi la fonction h(x) diff√®re et la fonction IC(n) simule selon une loi $Exp(1)+4$.

Sur la Figure 8, cette seconde m√©thode de Monte Carlo semble converger plus vite vers la valeur exacte, n√©anmoins, les intervalles de confiance ne contiennent que rarement la valeur exacte, ce qui n'est pas le cas dans la premi√®re m√©thode. On peut dire par ce sens que la seconde m√©thode est moins "fiable" que la premi√®re. On va √† pr√©sent chercher √† comparer ces 2 approximations sur la taille de l'√©chantillon n√©cessaire pour obtenir une m√™me pr√©cision, et le temps de calcul.


```{r}
#code R8
#Fonction qui utilise la m√©thode utilisant une loi exponentielle
MC_exp = function(n){
  tstart = Sys.time() #debut du chronom√©trage de la fonction
  #fonction h utilis√© dans la m√©thode utilisant une loi exponentielle
  h=function(x){
    y=-(x^2)/2 +x -4
    return(1/sqrt(2*pi)*exp(y))
  }

  X=4+rexp(n) # G√©n√©ration de n tirages i.i.d. E(1)+4

  tend = Sys.time() #fin du chronom√©trage de la fonction
  return(c(mean(h(X)),tend-tstart)) # Retourne la moyenne empirique de niveau n et le temps
}

#Fonction qui utilise la m√©thode utilisant une loi normale
MC_norm = function(n){
  tstart = Sys.time() #debut du chronom√©trage de la fonction
  #fonction h utilis√© dans la m√©thode utilisant une loi exponentielle
  h=function(x){
    return(as.numeric(x>4))
  }

  X=rnorm(n) # G√©n√©ration de n tirages i.i.d. N(0,1)

  tend = Sys.time() #fin du chronom√©trage de la fonction
  return(c(mean(h(X)),tend-tstart)) # Retourne la moyenne empirique de niveau n et le temps
}

N=10^(4:8)
val_exacte = pnorm(q=4,lower.tail = F)# Calcul de la valeur exacte

#resultat pour la premi√®re m√©thode
resultat_norm = sapply(N,MC_norm) #approximation et temps pour diff√©rents n
resultat_norm = rbind(abs(resultat_norm[1,]-val_exacte),resultat_norm)

rownames(resultat_norm)=c("Erreur d'approximation",
                          "Approximation de p",
                          "Temps de calcul (s)")
colnames(resultat_norm)=paste0("n=", N)

cat("\nR√©sultat pour l'approximation utilisant une loi normale\n")
print(resultat_norm)

#resultat pour la deuxi√®me m√©thode
resultat_exp = sapply(N,MC_exp) #approximation et temps pour diff√©rents n
resultat_exp = rbind(abs(resultat_exp[1,]-val_exacte),resultat_exp)

rownames(resultat_exp)=c("Erreur d'approximation",
                         "Approximation de p",
                         "Temps de calcul (s)")
colnames(resultat_exp)=paste0("n=", N)

cat("\nR√©sultat pour l'approximation utilisant une loi exponentielle\n")
print(resultat_exp)
```

Ce code contient 2 fonctions analogue : **MC_exp(n)** et **MC_norm(n)** qui execute les m√©thodes de Monte Carlo utilisant respectivement la loi exponentielle et la loi normale. Ces fonctions renvoient l'approximation de niveau n est le temps de calcul que ctte derni√®re a cout√©e. Nous testons alors les 2 m√©thodes pour les puissances de 10 allant de $10^4$ √† $10^8$. Cela nous permet de construire un tableau contenant sur les colonnes les valeurs de n, et sur les lignes les erreur d'approximation, les valeurs d'approximations, et le temps de calcul.

Passons √† l'analyse des r√©sultats. En ce qui concerne les temps de calculs, ils sont du m√™me ordre de grandeur qu'importe la taille de n. Cependant la pr√©cision est bien plus importante pour la seconde m√©thode. Tandis que la m√©thode utilisant une simulation de loi normale peine √† √©stimer p √† $10^{-7}$ pr√™t, m√™me lorsque n vaut $10^8$; la m√©thode utilisant une simulation de loi exponentielle n'√©prouve m√™me pas le besoin d'un n=$10^4$ pour atteindre une telle pr√©cision. Et dans le m√™me temps que la premi√®re m√©thode, elle permet une pr√©cision de l'odre du milliardi√®me !

---
##Conclusion
---



  Dans la premi√®re partie, nous avons illustr√© les diff√©rents modes de convergence des variables al√©atoires. L'√©tude de $\hat{b}_n$ a confirm√© les Lois Faible et Forte des Grands Nombres (LFGN et LGFN), montrant que la moyenne empirique converge de mani√®re fiable vers l'esp√©rance th√©orique.

  L'analyse du maximum cumul√© $M_n$ (Code R2) a valid√© la convergence presque s√ªre. Le contre-exemple de la suite de Bernoulli $X_n \sim \text{Ber}(1/\sqrt{n})$ (Code R3) a servi √† √©montrer que la convergence en probabilit√© est une condition plus faible qui n'implique pas n√©cessairement la convergence presque s√ªre.
  
 Finalement, l'illustration du Th√©or√®me Central Limite (TCL) sur la loi du $\chi^2$ (Code R5) a justifi√© l'usage de la loi normale pour la construction des intervalles de confiance.

La seconde partie, d√©di√©e √† l'int√©gration par les m√©thodes de Monte Carlo (MC), a √©tabli une r√®gle essentielle : l'efficacit√© et la validit√© des estimations d√©pendent de la finitude de la variance $\mathbb{E}_f [h(X)^2]$.

L'√©tude avec la densit√© de Cauchy (Code R7) a montr√© que lorsque la variance est finie, les intervalles de confiance (IC) fonctionnent parfaitement et se r√©tr√©cissent comme l'exige le TCL.

Inversement, l'analyse avec la densit√© de Laplace (Code R7) a d√©montr√© que, m√™me si la LFGN garantie la convergence de l'estimation vers la valeur exacte, l'infinit√© de la variance emp√™che l'application du TCL, rendant les IC invalides ou trompeurs.

Enfin, pour estimer la probabilit√© $\mathbb{P}(X>4)$, nous avons prouv√© que l'√âchantillonnage Pr√©f√©rentiel (Code R8) est tr√®s avantageux. En ajustant la loi de tirage (Loi Exponentielle d√©cal√©e), nous avons r√©duit la variance de l'estimateur, obtenant ainsi une pr√©cision bien sup√©rieure √† celle de la m√©thode standard.
